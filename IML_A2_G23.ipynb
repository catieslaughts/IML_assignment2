{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2 \n",
    "\n",
    "Authors: Catherine Slaughter and Paloma Jol of Group 23  \n",
    "Assignment 2 of the course introduction to machine learning (IML)  \n",
    "Due: 23 december 2022 23:59\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import astropy\n",
    "import astropy.units as u\n",
    "import astropy.coordinates as coord\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import umap.umap_ as umap \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start pre-processing data, and exploring data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_data(filename = 'data/A2_data.csv'):\n",
    "    '''Reads in the data from the given csv, and saves it in a Pandas dataframe'''\n",
    "    df = pd.read_csv(filename)\n",
    "    X,y= df.loc[:, df.columns != 'class'], df['class']\n",
    "    return df, X, y\n",
    "\n",
    "def train_test_split_drop(X,y, test_size : float, random_state=42, drop=True):\n",
    "    '''Split the test and training data using a random state, if drop is True the index are reset after'''\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=test_size, random_state=42)\n",
    "    X_train,y_train=X_train.reset_index(drop=drop),y_train.reset_index(drop=drop)\n",
    "    X_test,y_test=X_test.reset_index(drop=drop),y_test.reset_index(drop=drop)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def histogram(column):\n",
    "    '''Plot the histograms of the features'''\n",
    "    plt.hist(X[column],facecolor='blue', alpha=0.8,histtype='bar', ec='black')\n",
    "    plt.title(column)\n",
    "    plt.xlabel('Feature label')\n",
    "    plt.ylabel('Number of samples')\n",
    "    plt.show()\n",
    "\n",
    "def feature_importances_plot(forest, X_data, title = 'Feature Importances Using MDI', ymax = None):\n",
    "    '''Plot random forest feature importances'''\n",
    "    importances = forest.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "\n",
    "    feature_names = X_data.columns\n",
    "    forest_importances = pd.Series(importances, index=feature_names)\n",
    "\n",
    "    x_locs = range(importances.size)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    forest_importances.plot.bar(yerr=std, ax=ax, capsize = 3)\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel('Mean Decrease in Impurity')\n",
    "    ax.set_ylim(0,ymax)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/\"\n",
    "\n",
    "data, X, y =read_in_data(path+'A2_data.csv')\n",
    "\n",
    "print(f'There are {X.shape[0]} samples of each {X.shape[1]} features')\n",
    "\n",
    "corr_data = X.copy()#for the later correlation plot\n",
    "\n",
    "#Data includes some identifiers lets remove those \n",
    "ID_parameters= ['field_ID','MJD','plate','alpha', 'delta']\n",
    "for ID in ID_parameters:\n",
    "    X=X.loc[:, X.columns != ID]\n",
    "\n",
    "histogram('u')\n",
    "\n",
    "#We know that a flux should be positive so remove the datapoint which does not have that\n",
    "#This datapoint has value -9999 so not a detection but a instrumentation issue\n",
    "I_remove=np.where(X['u']<0)[0]\n",
    "print(data.loc[I_remove])\n",
    "X,y = X.drop(I_remove),y.drop(I_remove)\n",
    "X,y=X.reset_index(drop=True),y.reset_index(drop=True)\n",
    "data_processed= X.join(y)\n",
    "data_processed.to_csv(path+'A2_data_preprocessed.csv', index=False)\n",
    "\n",
    "corr_data = corr_data.drop(I_remove)\n",
    "corr_data = corr_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(f'There are final {X.shape[0]} samples of now {X.shape[1]} features')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split_drop(X,y, test_size=0.33, random_state=42, drop=True)\n",
    "\n",
    "\n",
    "print(f'There are {X_train.shape[0]} training samples of each {X_train.shape[1]} features')\n",
    "\n",
    "for C in X.columns:\n",
    "    histogram(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sky-Map Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra = coord.Angle(data['alpha']*u.degree)\n",
    "ra = ra.wrap_at(180*u.degree)\n",
    "dec = coord.Angle(data['delta']*u.degree)\n",
    "\n",
    "new_df_arr = np.array([ra,dec,data['class']])\n",
    "plot_df = pd.DataFrame(np.transpose(new_df_arr), columns = ['ra','dec','class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,12))\n",
    "colors = ['red', 'green', 'blue']\n",
    "\n",
    "for idx, cls in enumerate(['GALAXY', 'QSO', 'STAR']):\n",
    "    loc = 311+idx\n",
    "    ax = fig.add_subplot(loc, projection=\"mollweide\")\n",
    "    ra_sub = plot_df.loc[plot_df['class'] == cls, 'ra']*u.degree.to(u.rad)\n",
    "    dec_sub = plot_df.loc[plot_df['class'] == cls, 'dec']*u.degree.to(u.rad)\n",
    "    \n",
    "    ax.scatter(ra_sub, dec_sub, s = 5, c=colors[idx])\n",
    "    ax.set_xticklabels(['14h','16h','18h','20h','22h','0h','2h','4h','6h','8h','10h'])\n",
    "    ax.grid(True)\n",
    "    ax.set_title(cls)\n",
    "    \n",
    "plt.tight_layout()    \n",
    "#plt.savefig('./figures/skymaps.pdf', dpi=300, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corr = corr_data.corr()\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(corr,cmap='coolwarm', vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "ticks = np.arange(0,len(corr_data.columns),1)\n",
    "ax.set_xticks(ticks)\n",
    "plt.xticks(rotation=90)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(corr_data.columns)\n",
    "ax.set_yticklabels(corr_data.columns)\n",
    "ax.set_title('Feature Correlation Plot', y = -0.1)\n",
    "#plt.savefig('./figures/correlationplot.pdf', dpi=300, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair Plot\n",
    "\n",
    "_This takes a second to run!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairplot_data = X.copy()\n",
    "pairplot_data['class'] = y\n",
    "color_pallete = {\n",
    "    'GALAXY' : 'red',\n",
    "    'QSO' : 'green',\n",
    "    'STAR' : 'blue'\n",
    "    \n",
    "}\n",
    "\n",
    "sns.pairplot(pairplot_data, hue = 'class', palette = color_pallete)\n",
    "#plt.savefig('./figures/pairplot.pdf', dpi=300, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start dimensionality reductions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualise the dimension models\n",
    "def visualise_components(model_name, y, finaldim, save=False):\n",
    "    fig = plt.figure(figsize = (8,8))\n",
    "    ax = fig.add_subplot(1,1,1) \n",
    "    ax.set_xlabel('Component 1', fontsize = 15)\n",
    "    ax.set_ylabel('Component 2', fontsize = 15)\n",
    "    ax.set_title(f'2 component {model_name}', fontsize = 20)\n",
    "    targets = np.unique(y)\n",
    "    colors = ['r', 'g', 'b','k','y']\n",
    "    for target, color in zip(targets,colors):\n",
    "        indicesToKeep = finaldim['class'] == target\n",
    "        ax.scatter(finaldim.loc[indicesToKeep, 'component 1']\n",
    "                , finaldim.loc[indicesToKeep, 'component 2']\n",
    "                , c = color\n",
    "                , s = 10\n",
    "                , marker = '.'\n",
    "                , alpha=0.2)\n",
    "    ax.legend(targets, fontsize=15)\n",
    "    ax.grid()\n",
    "    if save == True:\n",
    "        plt.savefig(f'plots/{model_name}.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in preprocessed data\n",
    "path = \"data/\"\n",
    "preprocessed, X, y = read_in_data(path+'A2_data_preprocessed.csv')\n",
    "X_train, X_test, y_train, y_test = train_test_split_drop(X,y, test_size=0.33, random_state=42, drop=True) #random state chosen for reproducable results\n",
    "\n",
    "x = X_train\n",
    "x = StandardScaler().fit_transform(x)\n",
    "\n",
    "params_list=[{\n",
    "    \"n_components\": 2,\n",
    "    \"svd_solver\": 'auto'\n",
    "},{\n",
    "     \"n_components\": 5,\n",
    "    \"svd_solver\": 'auto'\n",
    "}]\n",
    "\n",
    "\n",
    "# Use PCA to reduce to two and to five dimensions for testing in RF\n",
    "for params in params_list:\n",
    "    pca = PCA(**params)\n",
    "    c = params['n_components']\n",
    "    principalComponents = pca.fit_transform(x)\n",
    "    principal = pd.DataFrame(data = principalComponents,columns = np.array([f'component {i}' for i in range(1,c+1)],dtype=str))\n",
    "    finalPca = pd.concat([principal, y_train], axis = 1)\n",
    "\n",
    "    visualise_components('PCA', y_train, finalPca) #only the first two components are shown for the five components\n",
    "\n",
    "    finalPca.to_csv(f'PCA_reduced_{c}_alphadelta.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiments run on UMAP since this method had visaully the best results and was faster to run than TSNE\n",
    "params_list=[{\n",
    "    \"n_neighbors\": 2,\n",
    "    \"min_dist\": 0.3,\n",
    "    \"metric\": \"correlation\", \n",
    "    \"init\": \"random\"\n",
    "},{\n",
    "    \"n_neighbors\": 5,\n",
    "    \"min_dist\": 0.3,\n",
    "    \"metric\": \"correlation\", \n",
    "    \"init\": \"random\"\n",
    "},{\n",
    "    \"n_neighbors\": 10,\n",
    "    \"min_dist\": 0.3,\n",
    "    \"metric\": \"correlation\", \n",
    "    \"init\": \"random\"\n",
    "},{\n",
    "    \"n_neighbors\": 50,\n",
    "    \"min_dist\": 0.3,\n",
    "    \"metric\": \"correlation\", \n",
    "    \"init\": \"random\"\n",
    "},{\n",
    "    \"n_neighbors\": 100,\n",
    "    \"min_dist\": 0.3,\n",
    "    \"metric\": \"correlation\", \n",
    "    \"init\": \"random\"\n",
    "},{\n",
    "    \"n_neighbors\": 200,\n",
    "    \"min_dist\": 0.3,\n",
    "    \"metric\": \"correlation\", \n",
    "    \"init\": \"random\"\n",
    "},{\n",
    "    \"n_neighbors\": 10,\n",
    "    \"min_dist\": 0.0,\n",
    "    \"metric\": \"correlation\", \n",
    "    \"init\": \"random\"\n",
    "},{\n",
    "    \"n_neighbors\": 10,\n",
    "    \"min_dist\": 0.1,\n",
    "    \"metric\": \"correlation\", \n",
    "    \"init\": \"random\"\n",
    "},{\n",
    "    \"n_neighbors\": 10,\n",
    "    \"min_dist\": 0.25,\n",
    "    \"metric\": \"correlation\", \n",
    "    \"init\": \"random\"\n",
    "},{\n",
    "    \"n_neighbors\": 10,\n",
    "    \"min_dist\": 0.4,\n",
    "    \"metric\": \"correlation\", \n",
    "    \"init\": \"random\"\n",
    "},{\n",
    "    \"n_neighbors\": 10,\n",
    "    \"min_dist\": 0.45,\n",
    "    \"metric\": \"correlation\", \n",
    "    \"init\": \"random\"\n",
    "},{\n",
    "    \"n_neighbors\": 10,\n",
    "    \"min_dist\": 0.5,\n",
    "    \"metric\": \"correlation\", \n",
    "    \"init\": \"random\"\n",
    "},{\n",
    "    \"n_neighbors\": 10,\n",
    "    \"min_dist\": 0.3,\n",
    "    \"metric\": \"correlation\", \n",
    "    \"init\": \"random\"\n",
    "},{\n",
    "    \"n_neighbors\": 10,\n",
    "    \"min_dist\": 0.3,\n",
    "    \"metric\": \"euclidean\", \n",
    "    \"init\": \"random\"\n",
    "},{\n",
    "    \"n_neighbors\": 10,\n",
    "    \"min_dist\": 0.3,\n",
    "    \"metric\": \"minkowski\", \n",
    "    \"init\": \"random\"\n",
    "},{\n",
    "    \"n_neighbors\": 10,\n",
    "    \"min_dist\": 0.3,\n",
    "    \"metric\": \"mahalanobis\", \n",
    "    \"init\": \"random\"\n",
    "},{\n",
    "    \"n_neighbors\": 10,\n",
    "    \"min_dist\": 0.3,\n",
    "    \"metric\": \"jaccard\", \n",
    "    \"init\": \"random\"\n",
    "},{\n",
    "    \"n_neighbors\": 10,\n",
    "    \"min_dist\": 0.3,\n",
    "    \"metric\": \"chebyshev\", \n",
    "    \"init\": \"random\"\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UMAP\n",
    "#best parameters from experiments\n",
    "params_list=[{\n",
    "    \"n_components\": 2,\n",
    "    \"n_neighbors\": 10,\n",
    "    \"min_dist\": 0.3,\n",
    "    \"metric\": \"correlation\", \n",
    "    \"init\": \"random\"\n",
    "},{\n",
    "    \"n_components\": 5,\n",
    "    \"n_neighbors\": 10,\n",
    "    \"min_dist\": 0.3,\n",
    "    \"metric\": \"correlation\", \n",
    "    \"init\": \"random\"\n",
    "}]\n",
    "for params in params_list:\n",
    "    print(params['n_neighbors'])\n",
    "    x = X_train\n",
    "    x = StandardScaler().fit_transform(x)\n",
    "    c = params['n_components']\n",
    "    reducer = umap.UMAP(**params)\n",
    "    UmapComponents = reducer.fit_transform(x)\n",
    "    UmapDf = pd.DataFrame(data = UmapComponents,columns = np.array([f'component {i}' for i in range(1,c+1)],dtype=str))\n",
    "    finalUmapDf = pd.concat([UmapDf, y_train], axis = 1)\n",
    "    visualise_components(f\"UMAP\", y_train, finalUmapDf)\n",
    "    finalUmapDf.to_csv(f'Umap_reduced_data_with_{c}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some experiments run on T-SNE to see the effect of changing perplexity\n",
    "params_list=[{\n",
    "    'perplexity' : 5 ,\n",
    "    'early_exaggeration' : 12.0 ,\n",
    "    'metric': 'euclidean'\n",
    "},{\n",
    "    'perplexity' : 30 ,\n",
    "    'early_exaggeration' : 12.0 ,\n",
    "    'metric': 'euclidean'\n",
    "},{\n",
    "    'perplexity' : 50 ,\n",
    "    'early_exaggeration' : 12.0 ,\n",
    "    'metric': 'euclidean'\n",
    "}]\n",
    "\n",
    "# Use T-SNE to reduce to two dimensions\n",
    "for params in params_list:\n",
    "    x = X_train\n",
    "    tsne = TSNE(n_components=2, verbose=1, perplexity=params['perplexity'], early_exaggeration=params['early_exaggeration'], metric=params['metric'])\n",
    "    tsneComponents = tsne.fit_transform(x)\n",
    "    tsneDf = pd.DataFrame(data = tsneComponents,columns = ['component 1', 'component 2'])\n",
    "    finalTsneDf = pd.concat([tsneDf, y_train], axis = 1)\n",
    "    finalTsneDf\n",
    "\n",
    "    visualise_components(f\"T-SNE_perplex{params['perplexity']}\", y_train, finalTsneDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cluster_object():\n",
    "    def __init__(self,method):\n",
    "        self.method= method \n",
    "\n",
    "    def cluster_fit(self,X,params):\n",
    "        clustering = self.method(**params)\n",
    "        clustering.fit(X)\n",
    "        return clustering\n",
    "\n",
    "    def score(self,y_pred, y):\n",
    "        nmis=0 #number misclassified\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] != y[i]:\n",
    "                nmis+=1\n",
    "        accuracy=nmis/len(y)\n",
    "        print(f'Out of {len(y)} there are {nmis} missclassified samples, thus an accuracy of {accuracy}')\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data import of dimension reduction\n",
    "path='data_last/'\n",
    "reduced_c2=pd.read_csv(path+'Umap_reduced_data.csv')\n",
    "visualise_components('Umap', reduced_c2['class'], reduced_c2)\n",
    "\n",
    "reduced_c5=pd.read_csv(path+'Umap_reduced_data_with_5.csv')\n",
    "visualise_components('Umap', reduced_c5['class'], reduced_c5)\n",
    "\n",
    "#reduced_pca=pd.read_csv(path+'PCA_reduced_5.csv')\n",
    "#visualise_components('PCA', reduced_pca['class'], reduced_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering the pre-processed data\n",
    "path = \"data/\"\n",
    "preprocessed, X, y = read_in_data(path+'A2_data_preprocessed.csv')\n",
    "X_train, X_test, y_train, y_test = train_test_split_drop(X,y, test_size=0.33, random_state=42, drop=True) #random state chosen for reproducable results\n",
    "X_scale=scale(X_train)\n",
    "Class_Names=np.unique(y_train)\n",
    "Kmeans=Cluster_object(KMeans)\n",
    "Kmean_cluster= Kmeans.cluster_fit(X_scale,params={'n_clusters': len(Class_Names), 'random_state':4})\n",
    "y_pred=Class_Names[Kmean_cluster.labels_]\n",
    "Kmeans_accuracy= Kmeans.score(y_pred,y_train)\n",
    "labels=pd.DataFrame(y_pred,columns=['class'])\n",
    "kmeans_Df=X.join(labels)\n",
    "\n",
    "\n",
    "#Clustering the reduced data\n",
    "reduced_c2, X, y= read_in_data(path+'Umap_reduced_data.csv')\n",
    "X_scale=scale(X)\n",
    "Class_Names=np.unique(y)\n",
    "Kmeans=Cluster_object(KMeans)\n",
    "Kmean_cluster= Kmeans.cluster_fit(X_scale,params={'n_clusters': len(Class_Names), 'random_state':1})\n",
    "y_pred=Class_Names[Kmean_cluster.labels_]\n",
    "Kmeans_accuracy= Kmeans.score(y_pred,y)\n",
    "labels=pd.DataFrame(Kmean_cluster.labels_,columns=['class'])\n",
    "kmeans_Df_reduced=X.join(labels)\n",
    "\n",
    "visualise_components('UMAP',Class_Names, reduced_c2)\n",
    "\n",
    "visualise_components('UMAP after K means', Kmean_cluster.labels_, kmeans_Df_reduced, save=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list= [{'eps':  0.001 , 'min_samples':  50.0 }, \n",
    "{'eps':  0.001 , 'min_samples':  1155.5555555555557 }, \n",
    "{'eps':  0.001 , 'min_samples':  2261.1111111111113 }, \n",
    "{'eps':  0.001 , 'min_samples':  3366.666666666667 }, \n",
    "{'eps':  0.001 , 'min_samples':  4472.222222222223 }, \n",
    "{'eps':  0.001 , 'min_samples':  5577.777777777778 }, \n",
    "{'eps':  0.001 , 'min_samples':  6683.333333333334 }, \n",
    "{'eps':  0.001 , 'min_samples':  7788.88888888889 }, \n",
    "{'eps':  0.001 , 'min_samples':  8894.444444444445 }, \n",
    "{'eps':  0.001 , 'min_samples':  10000.0 }, \n",
    "{'eps':  0.0064444444444444445 , 'min_samples':  50.0 }, \n",
    "{'eps':  0.0064444444444444445 , 'min_samples':  1155.5555555555557 }, \n",
    "{'eps':  0.0064444444444444445 , 'min_samples':  2261.1111111111113 }, \n",
    "{'eps':  0.0064444444444444445 , 'min_samples':  3366.666666666667 }, \n",
    "{'eps':  0.0064444444444444445 , 'min_samples':  4472.222222222223 }, \n",
    "{'eps':  0.0064444444444444445 , 'min_samples':  5577.777777777778 }, \n",
    "{'eps':  0.0064444444444444445 , 'min_samples':  6683.333333333334 }, \n",
    "{'eps':  0.0064444444444444445 , 'min_samples':  7788.88888888889 }, \n",
    "{'eps':  0.0064444444444444445 , 'min_samples':  8894.444444444445 }, \n",
    "{'eps':  0.0064444444444444445 , 'min_samples':  10000.0 }, \n",
    "{'eps':  0.01188888888888889 , 'min_samples':  50.0 }, \n",
    "{'eps':  0.01188888888888889 , 'min_samples':  1155.5555555555557 }, \n",
    "{'eps':  0.01188888888888889 , 'min_samples':  2261.1111111111113 }, \n",
    "{'eps':  0.01188888888888889 , 'min_samples':  3366.666666666667 }, \n",
    "{'eps':  0.01188888888888889 , 'min_samples':  4472.222222222223 }, \n",
    "{'eps':  0.01188888888888889 , 'min_samples':  5577.777777777778 }, \n",
    "{'eps':  0.01188888888888889 , 'min_samples':  6683.333333333334 }, \n",
    "{'eps':  0.01188888888888889 , 'min_samples':  7788.88888888889 }, \n",
    "{'eps':  0.01188888888888889 , 'min_samples':  8894.444444444445 }, \n",
    "{'eps':  0.01188888888888889 , 'min_samples':  10000.0 }, \n",
    "{'eps':  0.017333333333333333 , 'min_samples':  50.0 }, \n",
    "{'eps':  0.017333333333333333 , 'min_samples':  1155.5555555555557 }, \n",
    "{'eps':  0.017333333333333333 , 'min_samples':  2261.1111111111113 }, \n",
    "{'eps':  0.017333333333333333 , 'min_samples':  3366.666666666667 }, \n",
    "{'eps':  0.017333333333333333 , 'min_samples':  4472.222222222223 }, \n",
    "{'eps':  0.017333333333333333 , 'min_samples':  5577.777777777778 }, \n",
    "{'eps':  0.017333333333333333 , 'min_samples':  6683.333333333334 }, \n",
    "{'eps':  0.017333333333333333 , 'min_samples':  7788.88888888889 }, \n",
    "{'eps':  0.017333333333333333 , 'min_samples':  8894.444444444445 }, \n",
    "{'eps':  0.017333333333333333 , 'min_samples':  10000.0 }, \n",
    "{'eps':  0.02277777777777778 , 'min_samples':  50.0 }, \n",
    "{'eps':  0.02277777777777778 , 'min_samples':  1155.5555555555557 }, \n",
    "{'eps':  0.02277777777777778 , 'min_samples':  2261.1111111111113 }, \n",
    "{'eps':  0.02277777777777778 , 'min_samples':  3366.666666666667 }, \n",
    "{'eps':  0.02277777777777778 , 'min_samples':  4472.222222222223 }, \n",
    "{'eps':  0.02277777777777778 , 'min_samples':  5577.777777777778 }, \n",
    "{'eps':  0.02277777777777778 , 'min_samples':  6683.333333333334 }, \n",
    "{'eps':  0.02277777777777778 , 'min_samples':  7788.88888888889 }, \n",
    "{'eps':  0.02277777777777778 , 'min_samples':  8894.444444444445 }, \n",
    "{'eps':  0.02277777777777778 , 'min_samples':  10000.0 }, \n",
    "{'eps':  0.028222222222222225 , 'min_samples':  50.0 }, \n",
    "{'eps':  0.028222222222222225 , 'min_samples':  1155.5555555555557 }, \n",
    "{'eps':  0.028222222222222225 , 'min_samples':  2261.1111111111113 }, \n",
    "{'eps':  0.028222222222222225 , 'min_samples':  3366.666666666667 }, \n",
    "{'eps':  0.028222222222222225 , 'min_samples':  4472.222222222223 }, \n",
    "{'eps':  0.028222222222222225 , 'min_samples':  5577.777777777778 }, \n",
    "{'eps':  0.028222222222222225 , 'min_samples':  6683.333333333334 }, \n",
    "{'eps':  0.028222222222222225 , 'min_samples':  7788.88888888889 }, \n",
    "{'eps':  0.028222222222222225 , 'min_samples':  8894.444444444445 }, \n",
    "{'eps':  0.028222222222222225 , 'min_samples':  10000.0 }, \n",
    "{'eps':  0.033666666666666664 , 'min_samples':  50.0 }, \n",
    "{'eps':  0.033666666666666664 , 'min_samples':  1155.5555555555557 }, \n",
    "{'eps':  0.033666666666666664 , 'min_samples':  2261.1111111111113 }, \n",
    "{'eps':  0.033666666666666664 , 'min_samples':  3366.666666666667 }, \n",
    "{'eps':  0.033666666666666664 , 'min_samples':  4472.222222222223 }, \n",
    "{'eps':  0.033666666666666664 , 'min_samples':  5577.777777777778 }, \n",
    "{'eps':  0.033666666666666664 , 'min_samples':  6683.333333333334 }, \n",
    "{'eps':  0.033666666666666664 , 'min_samples':  7788.88888888889 }, \n",
    "{'eps':  0.033666666666666664 , 'min_samples':  8894.444444444445 }, \n",
    "{'eps':  0.033666666666666664 , 'min_samples':  10000.0 }, \n",
    "{'eps':  0.03911111111111111 , 'min_samples':  50.0 }, \n",
    "{'eps':  0.03911111111111111 , 'min_samples':  1155.5555555555557 }, \n",
    "{'eps':  0.03911111111111111 , 'min_samples':  2261.1111111111113 }, \n",
    "{'eps':  0.03911111111111111 , 'min_samples':  3366.666666666667 }, \n",
    "{'eps':  0.03911111111111111 , 'min_samples':  4472.222222222223 }, \n",
    "{'eps':  0.03911111111111111 , 'min_samples':  5577.777777777778 }, \n",
    "{'eps':  0.03911111111111111 , 'min_samples':  6683.333333333334 }, \n",
    "{'eps':  0.03911111111111111 , 'min_samples':  7788.88888888889 }, \n",
    "{'eps':  0.03911111111111111 , 'min_samples':  8894.444444444445 }, \n",
    "{'eps':  0.03911111111111111 , 'min_samples':  10000.0 }, \n",
    "{'eps':  0.04455555555555556 , 'min_samples':  50.0 }, \n",
    "{'eps':  0.04455555555555556 , 'min_samples':  1155.5555555555557 }, \n",
    "{'eps':  0.04455555555555556 , 'min_samples':  2261.1111111111113 }, \n",
    "{'eps':  0.04455555555555556 , 'min_samples':  3366.666666666667 }, \n",
    "{'eps':  0.04455555555555556 , 'min_samples':  4472.222222222223 }, \n",
    "{'eps':  0.04455555555555556 , 'min_samples':  5577.777777777778 }, \n",
    "{'eps':  0.04455555555555556 , 'min_samples':  6683.333333333334 }, \n",
    "{'eps':  0.04455555555555556 , 'min_samples':  7788.88888888889 }, \n",
    "{'eps':  0.04455555555555556 , 'min_samples':  8894.444444444445 }, \n",
    "{'eps':  0.04455555555555556 , 'min_samples':  10000.0 }, \n",
    "{'eps':  0.05 , 'min_samples':  50.0 }, \n",
    "{'eps':  0.05 , 'min_samples':  1155.5555555555557 }, \n",
    "{'eps':  0.05 , 'min_samples':  2261.1111111111113 }, \n",
    "{'eps':  0.05 , 'min_samples':  3366.666666666667 }, \n",
    "{'eps':  0.05 , 'min_samples':  4472.222222222223 }, \n",
    "{'eps':  0.05 , 'min_samples':  5577.777777777778 }, \n",
    "{'eps':  0.05 , 'min_samples':  6683.333333333334 }, \n",
    "{'eps':  0.05 , 'min_samples':  7788.88888888889 }, \n",
    "{'eps':  0.05 , 'min_samples':  8894.444444444445 }, \n",
    "{'eps':  0.05 , 'min_samples':  10000.0 } ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##DBSCAN\n",
    "#best parameters\n",
    "params_list=[{'eps': 0.05, 'min_samples': 50.0},\n",
    "{'eps': 0.04455555555555556, 'min_samples': 50.0},\n",
    "{'eps':0.2, 'min_samples': 1e4}]#last one illustrative gives only outliers as result\n",
    "\n",
    "Dbscan=Cluster_object(DBSCAN) \n",
    "\n",
    "for params_db in params_list:\n",
    "    Dbscan_cluster_reduced= Dbscan.cluster_fit(X_scale,params=params_db)\n",
    "    #y_pred=Class_Names[Dbscan_cluster_reduced.labels_] #Since often we do not have only 3 clusters this will be out of bounds\n",
    "    y_pred=Dbscan_cluster_reduced.labels_\n",
    "    print(params_db, len(np.unique(y_pred)))\n",
    "    \n",
    "    #Dbscan_accuracy= Dbscan.score(y_pred,y) #Acuuracy harder to judge with unequal number of clusters\n",
    "    labels_reduced=pd.DataFrame(y_pred,columns=['class'])\n",
    "    dbscan_Df_reduced=X.join(labels_reduced)\n",
    "\n",
    "    visualise_components('UMAP after dbscan',y_pred, dbscan_Df_reduced)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Tree Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split_drop(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = tree.DecisionTreeClassifier()\n",
    "cv_output = cross_validate(tree_clf, X_train, y_train, cv=5, return_train_score=True)\n",
    "print(\"Simple cross-validation yields average %0.5f accuracy +/- %0.5f\" % (cv_output['test_score'].mean(), \n",
    "                                                                           cv_output['test_score'].std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxdepth = np.arange(1, 21, 1)\n",
    "#maxdepth[0]+=1\n",
    "\n",
    "valmeans = []\n",
    "valstds = []\n",
    "\n",
    "trameans = []\n",
    "trastds = []\n",
    "\n",
    "for idx, depth in enumerate(maxdepth):\n",
    "    clf_tree = tree.DecisionTreeClassifier(max_depth = depth)\n",
    "    cv_output = cross_validate(clf_tree, X_train, y_train, cv=5, return_train_score=True)\n",
    "    \n",
    "    trameans.append(cv_output['train_score'].mean())\n",
    "    trastds.append(cv_output['train_score'].std())   \n",
    "    \n",
    "    valmeans.append(cv_output['test_score'].mean())\n",
    "    valstds.append(cv_output['test_score'].std()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(maxdepth, trameans, yerr=trastds, ecolor = 'black', capsize = 3, label = 'Training')\n",
    "plt.errorbar(maxdepth, valmeans, yerr=valstds, ecolor = 'black', capsize = 3, label = 'Validation')\n",
    "plt.xticks(maxdepth[1:])\n",
    "plt.title('Max Depth vs. Accuracies')\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend();\n",
    "\n",
    "#plt.savefig('./figures/trainvsvalidation.pdf', dpi=300, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter Tuning via Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_data=pd.read_csv(path+'PCA_reduced_5.csv') #read in data\n",
    "red_X,red_y= red_data.loc[:, red_data.columns != 'class'], red_data['class']\n",
    "\n",
    "#create training/test data for GSCV\n",
    "print(f'There are total {red_X.shape[0]} samples of {red_X.shape[1]} features')\n",
    "X_train, X_test, y_train, y_test = train_test_split_drop(red_X,red_y)\n",
    "print(f'There are {X_train.shape[0]} training samples of each {X_train.shape[1]} features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create pair plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates a neat little pairplot of the data, useful for visualization, though takes a bit to run\n",
    "pairplot_data = red_data\n",
    "pairplot_data['class'] = y\n",
    "color_pallete = {\n",
    "    'GALAXY' : 'red',\n",
    "    'QSO' : 'green',\n",
    "    'STAR' : 'blue'\n",
    "    \n",
    "}\n",
    "\n",
    "sns.pairplot(pairplot_data, hue = 'class', palette = color_pallete)\n",
    "#plt.savefig('./figures/reducedpairplot.pdf', dpi=300, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run GSCV\n",
    "_This takes a while!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 5 #number of features in the reduced data set\n",
    "\n",
    "params = { #dictionary of possible parameters\n",
    "    'criterion':  ['gini', 'entropy'],\n",
    "    'max_depth':  [6,7,8,9,10],\n",
    "    'max_features': np.linspace(1,n_features,3).astype(int).tolist(), #for a 5-feature dataset, this gives [1,3,5]\n",
    "    'max_samples': [.25,.5,.75,1.],\n",
    "    'n_estimators': [150]\n",
    "}\n",
    "\n",
    "forest_clf = GridSearchCV(estimator=ensemble.RandomForestClassifier(), param_grid=params, cv=5, n_jobs=5, verbose=1, \n",
    "                   return_train_score = True) #create gridsearchcv\n",
    "\n",
    "forest_clf.fit(X_train, y_train)\n",
    "forest_clf.best_params_ #output \"best\" parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull out important info.\n",
    "params_df = pd.DataFrame(forest_clf.cv_results_['params'])\n",
    "validation_df = pd.DataFrame(forest_clf.cv_results_['mean_test_score'], columns=['Validation'])\n",
    "training_df = pd.DataFrame(forest_clf.cv_results_['mean_train_score'], columns=['Training'])\n",
    "\n",
    "#put into single dataframe\n",
    "gscv_output = pd.concat([params_df,validation_df,training_df],axis=1)\n",
    "#calculate percent difference\n",
    "gscv_output['Percent Difference'] = (gscv_output['Training'] - gscv_output['Validation'])/gscv_output['Training']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save output dataframe, if wanted\n",
    "gscv_output.to_csv('data/reducedcrosscor_out.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Parameter Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bounds on the reduced hypothesis space\n",
    "minsearch = 0\n",
    "maxsearch = .05\n",
    "\n",
    "#create histogram of percent differences\n",
    "plt.hist(gscv_output['Percent Difference'], bins = 40);\n",
    "#plt.axvline(minsearch, c='grey', linestyle = 'dashed')\n",
    "plt.axvline(maxsearch, c='grey', linestyle = 'dashed', label = 'Max boundary')\n",
    "plt.title('Histogram of Percent Differences')\n",
    "plt.xlabel('Percent Difference')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "\n",
    "# plt.savefig('./figures/meanDiffHist.pdf', dpi=300, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose best parameters from this space\n",
    "selected_best = gscv_output.loc[(gscv_output['Percent Difference'] > minsearch) & (gscv_output['Percent Difference'] < maxsearch)].iloc[[gscv_output.loc[(gscv_output['Percent Difference'] > minsearch) & (gscv_output['Percent Difference'] < maxsearch), 'Validation'].argmax()]]\n",
    "selected_idx = selected_best.index.item() #index needed for later visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary of selected parameters\n",
    "selected_params = {'criterion': selected_best['criterion'].item(),\n",
    "                   'max_depth': int(selected_best['max_depth'].item()),\n",
    "                   'max_features': selected_best['max_features'].item(), \n",
    "                   'max_samples' : selected_best['max_samples'].item(),\n",
    "                   'n_estimators': selected_best['n_estimators'].item()}\n",
    "\n",
    "#create random forest\n",
    "selected = ensemble.RandomForestClassifier(**selected_params)\n",
    "\n",
    "selected_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the training and validation scores by index for all 120 options\n",
    "\n",
    "plt.plot(np.arange(len(gscv_output['Training'])), gscv_output['Training'], label = 'Training')\n",
    "plt.plot(np.arange(len(gscv_output['Validation'])), gscv_output['Validation'], label = 'Validation')\n",
    "#plt.plot(np.arange(len(gscv_output['Percent Difference'])), gscv_output['Percent Difference'], label = 'Percent Difference')\n",
    "\n",
    "plt.axvline(selected_idx, c='darkgrey', linestyle = 'dashed', label = 'Selected Best Parameters')\n",
    "\n",
    "plt.title('Training and Validation Mean Score for All Parameter Combinations')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(.7,1.01)\n",
    "plt.legend();\n",
    "\n",
    "#plt.savefig('./figures/scoresLinePlot.pdf', dpi=300, bbox_inches = 'tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Averaged Parameter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create averaged parameter plot\n",
    "fig = plt.figure(figsize=(14,8))\n",
    "\n",
    "xlabels = ['gini','entropy']\n",
    "x_axis_locs = np.arange(len(xlabels))\n",
    "parameterlist = ['criterion', 'max_depth', 'max_features', 'max_samples']\n",
    "\n",
    "for idx, param in enumerate(parameterlist):\n",
    "    loc = 221+idx\n",
    "    ax = fig.add_subplot(loc)\n",
    "    \n",
    "    tra_means = []\n",
    "    tra_stds = []\n",
    "    val_means = []\n",
    "    val_stds = []\n",
    "    for value in gscv_output[param].unique():\n",
    "        tra_means.append(gscv_output[gscv_output[param] == value]['Training'].mean())\n",
    "        tra_stds.append(gscv_output[gscv_output[param] == value]['Training'].std())\n",
    "        val_means.append(gscv_output[gscv_output[param] == value]['Validation'].mean())\n",
    "        val_stds.append(gscv_output[gscv_output[param] == value]['Validation'].std())\n",
    "    \n",
    "    if idx == 0:\n",
    "        #ax.bar(x_axis_locs-.24, tra_means, width=.47, label = 'Training Score', yerr = tra_stds, capsize = 3)\n",
    "        ax.bar(x_axis_locs, val_means, width=.99, label = 'Validation Score', yerr = val_stds, capsize = 3)\n",
    "        ax.set_xticks(x_axis_locs, xlabels)\n",
    "        ax.set_ylim(0.82,0.92)\n",
    "    \n",
    "    else:\n",
    "        #ax.errorbar(gscv_output[param].unique(), tra_means, label = 'Training Score')#, yerr = tra_stds, capsize = 3,ecolor='black')\n",
    "        ax.errorbar(gscv_output[param].unique(), val_means, label = 'Validation Score')#, yerr = val_stds, capsize = 3,ecolor='black')\n",
    "        ax.set_xticks(gscv_output[param].unique())\n",
    "        #ax.set_ylim(0.82,0.92)\n",
    "        \n",
    "    ax.set_xlabel('Value')\n",
    "    ax.set_ylabel('Mean Accuracy')\n",
    "    ax.set_title(param)\n",
    "    #ax.legend()\n",
    "    \n",
    "plt.suptitle('Parameter Value vs. Validation Score')\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.savefig('./figures/GSCVout.pdf', dpi=300, bbox_inches = 'tight')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importances Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected.fit(red_X,red_y)\n",
    "feature_importances_plot(selected, red_X, title = 'Feature Importances for Selected Parameters', ymax = 1)\n",
    "# plt.savefig('./figures/FI_selected.png', dpi=300, bbox_inches = 'tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing n_estimators and max_samples from dictionary so we can make a single tree with the same other parameters\n",
    "tree_params = selected_params.copy()\n",
    "del tree_params[\"n_estimators\"]\n",
    "del tree_params[\"max_samples\"]\n",
    "tree_params\n",
    "\n",
    "clf_tree = tree.DecisionTreeClassifier(**tree_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 5\n",
    "\n",
    "selected_values = []\n",
    "tree_values = []\n",
    "    \n",
    "for i in range(n_runs):\n",
    "    X_train_avg, X_test_avg, y_train_avg, y_test_avg = train_test_split_drop(red_X,red_y)\n",
    "#     print('split')\n",
    "    selected.fit(X_train_avg, y_train_avg)\n",
    "#     print('selected trained')\n",
    "#     print('gscv trained')\n",
    "    tree_clf.fit(X_train_avg, y_train_avg)\n",
    "#     print('tree trained')\n",
    "    \n",
    "    selected_values.append(selected.score(X_test_avg, y_test_avg))\n",
    "    tree_values.append(tree_clf.score(X_test_avg, y_test_avg))\n",
    "    \n",
    "    print('Run '+str(i+1)+' complete')\n",
    "\n",
    "selected_values = np.asarray(selected_values)\n",
    "tree_values = np.asarray(tree_values)\n",
    "\n",
    "print(\"Selected Params: %0.5f accuracy +/- %0.5f over %d runs\" % (selected_values.mean(), selected_values.std(), n_runs))\n",
    "print(\"Single Tree Params: %0.5f accuracy +/- %0.5f over %d runs\" % (tree_values.mean(), tree_values.std(), n_runs))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "68c79ef49de77e4f4c39c080df3aacfbccba64cf5e2e2a491733511db187d0f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
